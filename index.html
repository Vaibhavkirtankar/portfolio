<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vaibhav Kirtankar - Portfolio</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <style>
        body {
            margin: 0;
            font-family: 'Calibri', sans-serif;
            color: #fff;
            background: #282c34;
            height: 100%;
            overflow: auto; /* Allow scrolling */
        }

        #particles-js {
            position: fixed;
            width: 100%;
            height: 100%;
            z-index: -1;
        }

        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: auto; /* Remove fixed height */
            padding: 20px;
        }

        h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        p {
            font-size: 1.2rem;
            margin-bottom: 1.5rem;
        }

        .profile-img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            margin-bottom: 1.5rem;
        }

        .resume-details {
            text-align: left;
            max-width: 900px;
            margin-top: 20px;
            background-color: #333;
            padding: 20px;
            border-radius: 8px;
            line-height: 1.6;
        }

        .resume-details h2 {
            margin-bottom: 10px;
            font-size: 1.8rem;
            color: #61dafb;
        }

        .resume-details p {
            margin: 10px 0;
            font-size: 1rem;
        }

        .resume-details ul {
            margin-left: 20px;
            list-style-type: disc;
        }

        .resume-details li {
            margin: 5px 0;
        }
    </style>
</head>
<body>
    <div id="particles-js"></div>
    <div class="container">
        <img src="images/image.jpeg" alt="Vaibhav Kirtankar" class="profile-img">
        <style>
            .profile-img {
                width: 250px; /* Adjust the size as needed */
                height: 250px; /* Ensures the image is square */
                object-fit: cover; /* Ensures the image fills the frame */
                border-radius: 10px; /* Add rounded corners (optional) */
                border: 2px solid #000000; /* Optional border for styling */
                margin: 20px; /* Add some spacing */
            }
        </style>
        <h1>Vaibhav Kirtankar</h1>
        <p>Data Engineer | Cloud Specialist</p>
        <style>
            .resume-details {
                margin-top: 0; /* Removes the top margin of the div */
                padding-top: 0; /* Removes the top padding of the div */
            }
        
            .resume-details img {
                display: block; /* Ensures image does not leave any space */
                margin-top: 0; /* Removes any margin above the image */
            }
        
            h2, p {
                margin-top: 0; /* Ensures no extra space on top of h2 or p */
                padding-top: 0;
            }
        
            /* Additional styles for other elements to maintain consistency */
            h2 {
                margin-bottom: 10px;
            }
        
            p {
                margin-bottom: 15px;
            }
        </style>
        
        <!-- Resume Details Section -->
        <div class="resume-details">
                <img
                    width="634"
                    height="4"
                    src="file:////Users/vaibhavkirtankar/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image001.png"
                />            
                <h2>Contact Information</h2>
                <p>Email: Vaibhavkirtankar26@gmail.com | Mobile: (214) 699-9434</p>
                <h2>Professional Summary</h2>
                <p>• Data Engineer with 6 years of experience designing, implementing, and optimizing large-scale data pipelines and migration projects using cloud platforms such as AWS and Microsoft Azure.</p>
                <p>• Proven expertise in developing ETL workflows, leveraging tools like AWS Glue, Pentaho Data Integration, and Informatica, to process and manage datasets exceeding 10 TB.</p>
                <p>• Skilled in database management and optimization, with hands-on experience in SQL technologies such as MySQL and MS SQL Server to enhance query performance by up to 35%.</p>
                <p>• Adept at implementing serverless architectures and automation workflows using AWS Lambda, Step Functions, and Azure Logic Apps to reduce manual intervention and improve system efficiency.</p>
                <p>• Strong problem-solving abilities, demonstrated by reducing storage costs by 20% through lifecycle policy automation and streamlining pipeline bottlenecks to achieve a 50% increase in processing speed.</p>
                <p>• Proficient in data visualization and reporting using Tableau, Power BI, Matplotlib, and Seaborn, enabling data-driven decision-making and operational efficiency improvements.</p>
                <p>• Experienced in cloud migrations, leading the successful transition of complex environments from Azure to AWS while ensuring zero downtime and operational continuity.</p>
                <p>• Skilled in securing data pipelines and infrastructure using encryption, IAM policies, and role-based access control, ensuring compliance with security standards.</p>
                <p>• Collaborative team player with a proven ability to train and mentor team members on advanced cloud technologies, fostering a culture of continuous learning and productivity.</p>
                
                <h2>Technical Skills</h2>
                <ul>
                    <li><strong>SQL Technologies:</strong> MySQL, MS SQL Server, PostgreSQL, Snowflake</li>
                    <li><strong>Languages:</strong> Python, SQL, R</li>
                    <li><strong>ETL Tools:</strong> Pentaho Data Integration, AWS Glue, Informatica</li>
                    <li><strong>IDE & Tools:</strong> Visual Studio, PyCharm, Jupyter Notebook, IntelliJ IDEA</li>
                    <li><strong>Source Control:</strong> JIRA, Trello, Asana</li>
                    <li><strong>Version Control:</strong> Git, Docker</li>
                    <li><strong>Cloud Technologies:</strong> AWS (Lambda, DynamoDB, S3, EC2, EMR, RDS), MS Azure (Azure Databricks, ADF, Azure Data Explorer, Azure HDInsight, ADLS)</li>
                    <li><strong>Visualization Tools:</strong> Tableau, Power BI, Matplotlib, Seaborn, Pandas</li>
                    <li><strong>Big Data Technologies:</strong> Apache Spark, Hadoop, Databricks</li>
                    <li><strong>Data Warehousing:</strong> Redshift, Snowflake, Azure Synapse Analytics</li>
                </ul>
                
                <h2>Professional Experience</h2>
                
                <p><strong>Client: Dell Technologies</strong> <span style="float: right;">Jan 2024 – Present</span>, Round Rock, TX</p>
                <p><strong>Role:</strong> Data Engineer</p>
                <ul>
                    <li>Designed and maintained ETL pipelines for real-time insights into customer support data.</li>
                    <li>Analyzed logs, ticket resolutions, and metrics to improve issue resolution strategies.</li>
                    <li>Automated ticket categorization and escalation workflows using Dell Boomi and AWS Lambda.</li>
                    <li>Built Power BI dashboards to visualize KPIs like resolution time and customer satisfaction.</li>
                    <li>Integrated data from CRM systems into centralized warehouses using Redshift and Snowflake.</li>
                    <li>Leveraged NLP techniques to analyze customer feedback and identify recurring issues.</li>
                    <li>Implemented predictive models to proactively address common hardware and software issues.</li>
                    <li>Ensured data integrity and security by adhering to global compliance standards.</li>
                    <li>Optimized cloud workflows on AWS S3, Glue, and CloudWatch for cost efficiency and scalability.</li>
                </ul>
                
                <p><strong>Client: Blue Cross Blue Shield</strong>, Durham, NC <span style="float: right;">Sep 2021 – Dec 2023</span></p>
                <p><strong>Role:</strong> Sr Data Engineer</p>
                <ul>
                    <li>Spearheaded the migration of 5 TB of databases from Azure SQL to Amazon RDS, improving query performance and reducing latency issues.</li>
                    <li>Automated key aspects of the migration process using Python and AWS SDK, reducing overall project timelines by 25%.</li>
                    <li>Conducted an in-depth analysis of Azure services and designed equivalent AWS architectures, minimizing compatibility risks during the transition.</li>
                    <li>Configured EC2 instances with auto-scaling and load balancers, optimizing cost and performance for fluctuating workloads.</li>
                    <li>Built post-migration data pipelines leveraging AWS Glue, S3, and Athena, enabling real-time analytics for business reporting.</li>
                    <li>Trained the client’s internal team on AWS services, helping reduce dependency on external support by 50%.</li>
                    <li>Maintained system uptime throughout the migration, ensuring zero disruption to ongoing business operations.</li>
                    <li>Conducted detailed analytics using Pandas and Seaborn, providing actionable insights on data trends post-migration.</li>
                    <li>Collaborated with cross-functional teams via Asana, ensuring alignment and timely delivery of migration milestones.</li>
                </ul>
                
                <p><strong>Client: Synchrony Financial</strong>, Stamford, CT <span style="float: right;">Apr 2020 – Aug 2021</span></p>
                <p><strong>Role:</strong> Data Engineer</p>
                <ul>
                    <li>Designed and maintained Azure Data Factory pipelines, processing over 5 million records daily with 99.8% uptime.</li>
                    <li>Reduced storage costs by 20% by applying intelligent tiering and automated lifecycle policies in Azure Blob Storage.</li>
                    <li>Created a high-performing data warehouse in Azure Synapse Analytics, slashing query times from 10 minutes to under 2 minutes.</li>
                    <li>Automated manual workflows using Azure Logic Apps and Functions, saving approximately 100 hours of effort every month.</li>
                    <li>Used Azure Monitor to proactively detect and fix pipeline issues, reducing failures by 25% within the first quarter.</li>
                    <li>Created interactive Power BI dashboards, allowing business teams to explore insights with minimal training.</li>
                    <li>Built ETL workflows on Azure Data Factory integrated with SQL Server, achieving 99.9% data reliability.</li>
                    <li>Optimized data lake performance on Azure Data Lake Storage (ADLS), reducing storage costs by 25%.</li>
                    <li>Leveraged Apache Spark on Azure HDInsight for distributed data transformations, scaling workloads effectively.</li>
                    <li>Utilized JIRA for tracking pipeline issues and collaborating with global teams to ensure quick resolution.</li>
                </ul>
                
                <p><strong>Client: Zoho Corporation</strong>, HYD, INDIA <span style="float: right;">Feb 2019 – Mar 2020</span></p>
                <p><strong>Role:</strong> Data Engineer</p>
                <ul>
                    <li>Set up AWS resources, including EC2, S3, and RDS, for 10+ small-scale projects, maintaining uptime.</li>
                    <li>Created simple ETL workflows using AWS Glue and Step Functions, reducing manual data transformation efforts by 30%.</li>
                    <li>Deployed and managed 10+ static websites on S3 and CloudFront, achieving consistent sub-2-second page load times.</li>
                    <li>Implemented IAM roles and security policies, reducing unauthorized access attempts.</li>
                    <li>Designed beginner-level Tableau visualizations for small-scale projects, fostering data-driven decision-making.</li>
                    <li>Implemented basic data transformation tasks using Pandas, reducing manual effort by 25%.</li>
                    <li>Experimented with Hadoop clusters for hands-on learning of distributed storage and processing concepts.</li>
                    <li>Created lightweight ETL workflows using Pentaho, enabling data integration for smaller datasets.</li>
                    <li>Utilized Git for version control and Trello for task management, improving team coordination on projects.</li>
                </ul>
                
                <h2>Education</h2>
                <p><strong>Wichita State University, Masters in Data Science</strong>, Wichita, KS</p>
                



        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
    <script>
        particlesJS('particles-js', {
            "particles": {
                "number": { "value": 80, "density": { "enable": true, "value_area": 800 } },
                "color": { "value": "#ffffff" },
                "shape": { "type": "circle" },
                "opacity": { "value": 0.5 },
                "size": { "value": 3, "random": true },
                "line_linked": { "enable": true, "distance": 150, "color": "#ffffff", "opacity": 0.4, "width": 1 },
                "move": { "enable": true, "speed": 6, "direction": "none", "random": false, "straight": false, "out_mode": "out" }
            },
            "interactivity": {
                "events": { "onhover": { "enable": true, "mode": "repulse" }, "onclick": { "enable": true, "mode": "push" } },
                "modes": { "repulse": { "distance": 200 }, "push": { "particles_nb": 4 } }
            },
            "retina_detect": true
        });
    </script>
</body>
</html>
